{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2_mixed_precision_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC77xxf9UOXo",
        "colab_type": "text"
      },
      "source": [
        "> 시작에 앞서...  \n",
        "> Colab을 [연결]한 뒤 [런타임] 탭에서 [런타임 유형 변경]을 TPU 또는 GPU로 바꿔 주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1-YQNRfQCSY",
        "colab_type": "text"
      },
      "source": [
        "**Tensorflow 2.0 에서 실습하는 float16 모델 생성 및 학습**\n",
        "=================\n",
        "\n",
        "#Overview\n",
        "1. Tensorflow2.0 에서 floating point precision을 결정하는 [dtype policy()](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/Policy)에 대해 이해합니다.\n",
        "2. float16/mixed_float16/mixed_bfloat16 방식으로 모델을 생성하고 학습해 봅니다.\n",
        "3. loss scale을 이해하고, 위의 Policy에서 자동으로 설정되지 않은 loss scale을 직접 설정해 봅니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzmH2bkEP3ns",
        "colab_type": "code",
        "outputId": "5103e6e3-58d5-49dd-f261-239c899b0f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-rc2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J26-IQn_T5Jj",
        "colab_type": "text"
      },
      "source": [
        "# 16bit floating point를 사용하는 이유\n",
        "\n",
        "> _python의 기본적인 dtype 및 연산 방법은 모두 float32에 최적화 되어 있고, 정확도도 낮은 16bit를 굳이 왜 ...?_  \n",
        "\n",
        "1. **학습 시간의 단축**\n",
        "  - 16bit floating point 연산에 최적화 된 연산장치(Tensor Core)를 갖고 있는 경우 학습 시간의 단축 효과를 볼 수 있다.\n",
        "    - RTX line-up, Titan-V, V100, TPU\n",
        "  - (오해 no) 입력 데이터를 16bit로 준비할 필요는 없다. 오히려 numpy 등 연산 처리에서 비효율적.\n",
        "\n",
        "2. **모델 크기의 축소**\n",
        "  - 연산 자원이 부족한 edge computing device에서 인공신경망 모델을 활용하기 위함\n",
        "    - Ex) DCASE2020 Task1b는 모델 크기에 500KB 용량 제한을 둠 (float32 기준 125k params)\n",
        "  - 활용 bit 수를 절반으로 줄일 경우, 같은 메모리에 두 배 큰 노드 수를 가진 네트워크를 활용할 수 있게 됨."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpjHahW5ZmFU",
        "colab_type": "text"
      },
      "source": [
        "# 실습 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rU-qQ0kQLNI",
        "colab_type": "code",
        "outputId": "db60898e-0fce-4c6d-ad43-5c18c8230d62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "! nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 16 00:46:13 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hcj2eR7aDiE",
        "colab_type": "text"
      },
      "source": [
        "# Setting the dtype policy\n",
        "\n",
        "*dtype policy*는 아래 항목들에 대해 작용하는 global policy.  \n",
        "- 모델을 구성하는 각 layer들의 dtype\n",
        "- ('mixed_float16'의 경우) optimizer의 DynamicLossScaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piutklcKZ3Xq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = mixed_precision.Policy('float16')\n",
        "mixed_precision.set_policy(policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNdiAAgudm4T",
        "colab_type": "text"
      },
      "source": [
        "*dtype policy*는 layer를 구성하는 아래 두 가지 요소에 작용한다.\n",
        "- Compute dtype\n",
        "  - 연산 정확도에 대한 dtype\n",
        "  - (활용) float16으로 설정하여 연산 속도에서 이득을 가져갈 수 있다.\n",
        "- Variable dtype\n",
        "  - Layer를 구성하는 variable에 대한 dtype\n",
        "  - (활용) float16으로 설정하여 모델 용량에 대해 이득을 가져갈 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frCeBe2YdlAP",
        "colab_type": "code",
        "outputId": "eea716e9-1b9b-4797-8daf-d02adc90c661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Compute dtype: %s' % policy.compute_dtype)\n",
        "print('Variable dtype: %s' % policy.variable_dtype)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compute dtype: float16\n",
            "Variable dtype: float16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCHAHUAIeoG3",
        "colab_type": "text"
      },
      "source": [
        "설정할 수 있는 dtype police를 알아보면,\n",
        "- 'float32' (기본값)\n",
        "  - compute_dtype = float32\n",
        "  - variable_dtype = float32\n",
        "- 'float16' / 'bfloat16'\n",
        "  - compute_dtype = float16\n",
        "  - variable_dtype = float16\n",
        "- 'mixed_float16' / 'mixed_bfloat16'\n",
        "  - compute_dtype = float16\n",
        "  - variable_dtype = float32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqkP2Q6LeYhJ",
        "colab_type": "code",
        "outputId": "046f5a4e-34f3-4187-a711-9b33b59e312c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_policy(policy)\n",
        "print('Compute dtype: %s' % policy.compute_dtype)\n",
        "print('Variable dtype: %s' % policy.variable_dtype)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compute dtype: float16\n",
            "Variable dtype: float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw9jEMBQfjgu",
        "colab_type": "text"
      },
      "source": [
        "# Building a toy model\n",
        "\n",
        "2개의 fully-connected(Dense) layer로 구성된 간단한 모델을 만들어 본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEtTgqkRfh7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape=(784,), name='digits')\n",
        "num_units = 4096\n",
        "\n",
        "dense1 = layers.Dense(num_units, activation='relu', name='dense_1')\n",
        "x = dense1(inputs)\n",
        "dense2 = layers.Dense(num_units, activation='relu', name='dense_2')\n",
        "x = dense2(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZVgZp9Bgw7t",
        "colab_type": "text"
      },
      "source": [
        "모델의 각 layer는 dtype에 대한 policy를 가지며, 기본적으로 global dtype policy를 따른다.\n",
        "\n",
        "즉, set_policy('mixed_float16')에서는\n",
        "1. 각 layer의 variable은 float32\n",
        "2. 모델에 입력되는 데이터는 float32 (첫 번째 layer에서 float16으로 cast)\n",
        "2. 각 layer의 출력은 float16\n",
        "\n",
        "으로 표현된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UglmB7ptgvXM",
        "colab_type": "code",
        "outputId": "6b8948b5-0292-4c87-fb8c-01df9c935cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# 1. print variable dtype of dense1\n",
        "print('dense1.kernel.dtype: %s' % dense1.kernel.dtype.name)\n",
        "# 2. print dtype of input data\n",
        "print('inputs.dtype: %s' % inputs.dtype.name)\n",
        "# 3. print dtype of output data\n",
        "print('x.dtype: %s' % x.dtype.name)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dense1.kernel.dtype: float32\n",
            "inputs.dtype: float32\n",
            "x.dtype: float16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQhO1BUAiTu1",
        "colab_type": "text"
      },
      "source": [
        "> *Q. set_policy('float16')에서는 1.2.3. 의 dtype이 어떻게 될까요?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLQ5nzmzi8xv",
        "colab_type": "text"
      },
      "source": [
        "## 출력 계층을 추가\n",
        "\n",
        "- 출력 Activation layer는 *'float32'*로 override할 것\n",
        "  - Activation layer에는 variable이 없기 때문\n",
        "\n",
        "> *Normally, you can create the output predictions as INCORRECT one, but this is not always numerically stable with float16.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D1MrbN_inSx",
        "colab_type": "code",
        "outputId": "ec8bb5f7-80be-4277-9695-d9c7840eda94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# INCORRECT: softmax and model output will be float16, when it should be float32\n",
        "# outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
        "\n",
        "# CORRECT: softmax and model output are float32\n",
        "x = layers.Dense(10, name='dense_logits')(x)\n",
        "outputs = layers.Activation('softmax', dtype='float32', name='predictions')(x)\n",
        "print('Outputs dtype: %s' % outputs.dtype.name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Outputs dtype: float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kldT371Hku8X",
        "colab_type": "text"
      },
      "source": [
        "(FYI) 모델의 마지막 계층이 Activation layer가 아닌 경우, 아래와 같이 모델의 출력을 float32로 cast해 줄 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qK0Vr-1jPIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The linear activation is an identity function. So this simply casts 'outputs'\n",
        "# to float32. In this particular case, 'outputs' is already float32 so this is a\n",
        "# no-op.\n",
        "\"\"\"outputs = layers.Activation('linear', dtype='float32')(outputs)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upodn4sYnNPU",
        "colab_type": "text"
      },
      "source": [
        "# Loss scaling\n",
        "\n",
        "'float16' 연산은 'float32'연산보다 dynamic range가 좁기 때문에 연산 과정에서 overflow/underflow가 발생하기 쉽다.  \n",
        "따라서 아래와 같이 backpropagation 과정에서 충분히 값이 전달될 수 있도록 적당히 큰 값을 곱해주는 loss scaling 과정이 필요하다.\n",
        "\n",
        "```\n",
        "loss_scale = 1024\n",
        "loss = model(inputs)\n",
        "loss *= loss_scale\n",
        "# We assume `grads` are float32. We do not want to divide float16 gradients\n",
        "grads = compute_gradient(loss, model.trainable_variables)\n",
        "grads /= loss_scale\n",
        "```\n",
        "\n",
        "'mixed_float16'에 대해서는 적당한 loss scale 값을 찾아주는 [DynamicLossScale](https://www.tensorflow.org/api_docs/python/tf/mixed_precision/experimental/DynamicLossScale)이 기본 loss scale로 설정된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40Q3A-Jalo6x",
        "colab_type": "code",
        "outputId": "b178fdca-7823-4e24-a13d-c14d424ed098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss_scale = policy.loss_scale\n",
        "print('Loss scale: %s' % loss_scale)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss scale: DynamicLossScale(current_loss_scale=32768.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0Zs9odPpMXD",
        "colab_type": "text"
      },
      "source": [
        "**'float16'에 대해서는 loss scale이 기본 적용 되어 있지 않다.**  \n",
        "이에 float16 연산에 최적화 되지 않은 일부 optimizer(ex. Adam)들은 학습이 제대로 이루어지지 않기에, 아래와 같이 직접 loss_scale='dynamic'을 지정해 줄 필요가 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir3J-KiwokKr",
        "colab_type": "code",
        "outputId": "073fdcdf-acd1-4029-871d-b8e62de309a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "new_policy = mixed_precision.Policy('float16')\n",
        "print('Loss scale: %s' % new_policy.loss_scale)\n",
        "new_policy = mixed_precision.Policy('float16', loss_scale='dynamic')\n",
        "print('Forcing dynamic loss scale: %s' % new_policy.loss_scale)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss scale: None\n",
            "Forcing dynamic loss scale: DynamicLossScale(current_loss_scale=32768.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTvE5SCvtW1j",
        "colab_type": "text"
      },
      "source": [
        "혹은 다음과 같이 optimizer 자체에 loss scaling을 적용하는 [LossScaleOptimizer](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer) 기능이 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFg63kQvor6D",
        "colab_type": "code",
        "outputId": "240c2801-bfd5-435b-fa59-91f80add25c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# optimizer = keras.optimizers.RMSprop()\n",
        "# optimizer = mixed_precision.LossScaleOptimizer(optimizer, loss_scale='dynamic')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"optimizer = mixed_precision.LossScaleOptimizer(optimizer, loss_scale='dynamic')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMbUDoP3bKH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI2DQ7oTvZnd",
        "colab_type": "text"
      },
      "source": [
        "# Let's Train model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI0wtJ0Oloe_",
        "colab_type": "code",
        "outputId": "21f5330a-631f-4076-e862-fbd64f75bdf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "x_test = x_test.reshape(10000, 784).astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5MeWnhOtu3Z",
        "colab_type": "code",
        "outputId": "8b1b1d2e-4b30-40c2-9381-8d2600697b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=8192,\n",
        "                    epochs=5,\n",
        "                    validation_split=0.2)\n",
        "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "print('Test loss:', test_scores[0])\n",
        "print('Test accuracy:', test_scores[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "6/6 [==============================] - 1s 166ms/step - loss: 4.7684 - accuracy: 0.4044 - val_loss: 0.5788 - val_accuracy: 0.8455\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 1s 130ms/step - loss: 0.6886 - accuracy: 0.7854 - val_loss: 0.3394 - val_accuracy: 0.8898\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 1s 127ms/step - loss: 0.3419 - accuracy: 0.8927 - val_loss: 0.2997 - val_accuracy: 0.9062\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 1s 127ms/step - loss: 0.3276 - accuracy: 0.8989 - val_loss: 0.1920 - val_accuracy: 0.9353\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 1s 131ms/step - loss: 0.2535 - accuracy: 0.9175 - val_loss: 0.1628 - val_accuracy: 0.9475\n",
            "313/313 - 1s - loss: 0.1776 - accuracy: 0.9443\n",
            "Test loss: 0.17762431502342224\n",
            "Test accuracy: 0.9442999958992004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFQ1roVzuCPn",
        "colab_type": "text"
      },
      "source": [
        "# GPU performance tips\n",
        "\n",
        "1. Batch size를 두 배로 키우세요\n",
        "  - float16 tensor를 사용할 경우, 메모리의 절반만 필요로 하기 때문에 모델 성능에 영향이 없다면 batch size를 키워서 학습 속도를 향상\n",
        "2. Tensor Core들을 활용할 수 있는 모델 구조를 만드세요\n",
        "  - 최신 NVIDIA GPU에 탑재된 Tensor core들은 float16의 matrix multiplication을 매우 빠르게 처리할 수 있음\n",
        "  - Tensor core들은 8의 배수로 구성된 tensor에서 동작하므로, 아래와 같은 모델 구조를 추천함\n",
        "```\n",
        "tf.keras.layers.Dense(units=64)\n",
        "# for Conv2d/Conv3d etc.\n",
        "tf.keras.layers.Conv2d(filters=48, kernel_size=7, stride=3)\n",
        "# for RNN layers\n",
        "tf.keras.layers.LSTM(units=64)\n",
        "tf.keras.Model.fit(epochs=2, batch_size=128)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DREE91DHvLR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYg1pgytXpe7",
        "colab_type": "text"
      },
      "source": [
        "# *References*\n",
        "https://www.tensorflow.org/guide/keras/mixed_precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXuL1G0DXt-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}